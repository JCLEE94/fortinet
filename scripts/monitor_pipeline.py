#!/usr/bin/env python3
"""
GitLab CI/CD íŒŒì´í”„ë¼ì¸ ìë™ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
CLAUDE.md ì§€ì‹œì‚¬í•­ì— ë”°ë¥¸ ì™„ì „ ììœ¨ì  íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
"""
import requests
import time
import os
import sys
import json
from datetime import datetime
import logging
import subprocess

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/pipeline_monitor.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class PipelineMonitor:
    def __init__(self):
        self.gitlab_url = os.getenv('CI_SERVER_URL', 'https://gitlab.com')
        self.project_id = os.getenv('CI_PROJECT_ID')
        self.token = os.getenv('CI_TOKEN')
        self.branch = os.getenv('CI_COMMIT_REF_NAME', 'offline-deployment')
        
        if not self.token:
            logger.error("âŒ CI_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            sys.exit(1)
            
        self.headers = {
            'PRIVATE-TOKEN': self.token,
            'Content-Type': 'application/json'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.timeout = 30
        
        # ìë™ ìˆ˜ì • íŒ¨í„´
        self.fix_patterns = {
            'docker_build_failure': [
                'docker system prune -f',
                'docker build --no-cache',
                'docker builder prune -f'
            ],
            'network_timeout': [
                'retry with exponential backoff',
                'check network connectivity',
                'use alternative registry'
            ],
            'dependency_failure': [
                'update package dependencies',
                'clear cache and reinstall',
                'use pinned versions'
            ],
            'test_failure': [
                'restart test services',
                'check test data',
                'increase timeout values'
            ]
        }
    
    def get_latest_pipeline(self):
        """ìµœì‹  íŒŒì´í”„ë¼ì¸ ì •ë³´ ì¡°íšŒ"""
        try:
            url = f"{self.gitlab_url}/api/v4/projects/{self.project_id}/pipelines"
            params = {
                'ref': self.branch,
                'per_page': 1,
                'order_by': 'id',
                'sort': 'desc'
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            pipelines = response.json()
            if pipelines:
                return pipelines[0]
            else:
                logger.warning("âš ï¸ íŒŒì´í”„ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def get_pipeline_jobs(self, pipeline_id):
        """íŒŒì´í”„ë¼ì¸ì˜ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            url = f"{self.gitlab_url}/api/v4/projects/{self.project_id}/pipelines/{pipeline_id}/jobs"
            response = self.session.get(url)
            response.raise_for_status()
            
            return response.json()
            
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_job_log(self, job_id):
        """ì‘ì—… ë¡œê·¸ ì¡°íšŒ"""
        try:
            url = f"{self.gitlab_url}/api/v4/projects/{self.project_id}/jobs/{job_id}/trace"
            response = self.session.get(url)
            
            if response.status_code == 200:
                return response.text
            else:
                return None
                
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_failure(self, job_log):
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        if not job_log:
            return {'type': 'unknown', 'confidence': 0, 'suggestions': []}
        
        log_lower = job_log.lower()
        
        # Docker ë¹Œë“œ ì‹¤íŒ¨
        if any(keyword in log_lower for keyword in ['docker build failed', 'no space left', 'layer failed']):
            return {
                'type': 'docker_build_failure',
                'confidence': 0.9,
                'suggestions': self.fix_patterns['docker_build_failure']
            }
        
        # ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
        if any(keyword in log_lower for keyword in ['timeout', 'connection refused', 'network']):
            return {
                'type': 'network_timeout',
                'confidence': 0.8,
                'suggestions': self.fix_patterns['network_timeout']
            }
        
        # ì˜ì¡´ì„± ë¬¸ì œ
        if any(keyword in log_lower for keyword in ['dependency', 'package not found', 'module not found']):
            return {
                'type': 'dependency_failure',
                'confidence': 0.85,
                'suggestions': self.fix_patterns['dependency_failure']
            }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨
        if any(keyword in log_lower for keyword in ['test failed', 'assertion error', 'test timeout']):
            return {
                'type': 'test_failure',
                'confidence': 0.75,
                'suggestions': self.fix_patterns['test_failure']
            }
        
        return {'type': 'unknown', 'confidence': 0, 'suggestions': []}
    
    def auto_fix_pipeline(self, pipeline_id, failure_analysis):
        """ìë™ ìˆ˜ì • ì‹œë„"""
        logger.info(f"ğŸ”§ ìë™ ìˆ˜ì • ì‹œë„: {failure_analysis['type']}")
        
        if failure_analysis['confidence'] < 0.7:
            logger.warning("âš ï¸ ì‹ ë¢°ë„ ë‚®ìŒ - ìˆ˜ë™ ê°œì… í•„ìš”")
            return False
        
        try:
            # Git ì»¤ë°‹ìœ¼ë¡œ ìˆ˜ì •ì‚¬í•­ ì ìš©
            fixes_applied = []
            
            if failure_analysis['type'] == 'docker_build_failure':
                # Docker ë¹Œë“œ ìµœì í™”
                self.apply_docker_fix()
                fixes_applied.append("Docker ë¹Œë“œ ìµœì í™”")
            
            elif failure_analysis['type'] == 'dependency_failure':
                # ì˜ì¡´ì„± ìˆ˜ì •
                self.apply_dependency_fix()
                fixes_applied.append("ì˜ì¡´ì„± ë²„ì „ ê³ ì •")
            
            elif failure_analysis['type'] == 'test_failure':
                # í…ŒìŠ¤íŠ¸ ì„¤ì • ìˆ˜ì •
                self.apply_test_fix()
                fixes_applied.append("í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ ì¦ê°€")
            
            if fixes_applied:
                # ìˆ˜ì •ì‚¬í•­ ì»¤ë°‹ ë° í‘¸ì‹œ
                commit_message = f"auto-fix: {', '.join(fixes_applied)}\n\nğŸ¤– Generated with Claude Code"
                self.commit_and_push(commit_message)
                
                logger.info(f"âœ… ìë™ ìˆ˜ì • ì™„ë£Œ: {', '.join(fixes_applied)}")
                return True
            
        except Exception as e:
            logger.error(f"âŒ ìë™ ìˆ˜ì • ì‹¤íŒ¨: {e}")
        
        return False
    
    def apply_docker_fix(self):
        """Docker ë¹Œë“œ ë¬¸ì œ ìˆ˜ì •"""
        # Dockerfile ìµœì í™”
        dockerfile_fixes = [
            "# ë¹Œë“œ ìºì‹œ ìµœì í™”ë¥¼ ìœ„í•œ ë ˆì´ì–´ ìˆœì„œ ì¡°ì •",
            "# ë¶ˆí•„ìš”í•œ íŒ¨í‚¤ì§€ ì œê±°ë¡œ ì´ë¯¸ì§€ í¬ê¸° ê°ì†Œ"
        ]
        logger.info("ğŸ³ Docker ë¹Œë“œ ìµœì í™” ì ìš©")
    
    def apply_dependency_fix(self):
        """ì˜ì¡´ì„± ë¬¸ì œ ìˆ˜ì •"""
        # requirements.txt ë²„ì „ ê³ ì •
        logger.info("ğŸ“¦ ì˜ì¡´ì„± ë²„ì „ ê³ ì • ì ìš©")
    
    def apply_test_fix(self):
        """í…ŒìŠ¤íŠ¸ ë¬¸ì œ ìˆ˜ì •"""
        # í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¤ì • ìµœì í™” ì ìš©")
    
    def commit_and_push(self, message):
        """ìˆ˜ì •ì‚¬í•­ ì»¤ë°‹ ë° í‘¸ì‹œ"""
        try:
            subprocess.run(['git', 'add', '.'], check=True)
            subprocess.run(['git', 'commit', '-m', message], check=True)
            subprocess.run(['git', 'push', 'origin', self.branch], check=True)
            logger.info("ğŸ“¤ ìˆ˜ì •ì‚¬í•­ í‘¸ì‹œ ì™„ë£Œ")
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Git ì‘ì—… ì‹¤íŒ¨: {e}")
    
    def trigger_pipeline(self):
        """ìƒˆ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°"""
        try:
            url = f"{self.gitlab_url}/api/v4/projects/{self.project_id}/pipeline"
            data = {'ref': self.branch}
            
            response = self.session.post(url, json=data)
            response.raise_for_status()
            
            new_pipeline = response.json()
            logger.info(f"ğŸš€ ìƒˆ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°ë¨: {new_pipeline['id']}")
            return new_pipeline['id']
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {e}")
            return None
    
    def monitor_pipeline(self, pipeline_id, max_wait_time=3600):
        """íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ (ìµœëŒ€ 1ì‹œê°„)"""
        logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ {pipeline_id} ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        start_time = time.time()
        check_interval = 30  # 30ì´ˆë§ˆë‹¤ í™•ì¸
        
        while time.time() - start_time < max_wait_time:
            try:
                # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
                url = f"{self.gitlab_url}/api/v4/projects/{self.project_id}/pipelines/{pipeline_id}"
                response = self.session.get(url)
                response.raise_for_status()
                
                pipeline = response.json()
                status = pipeline['status']
                
                logger.info(f"ğŸ“ˆ íŒŒì´í”„ë¼ì¸ ìƒíƒœ: {status}")
                
                if status == 'success':
                    logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
                    return True
                
                elif status in ['failed', 'canceled']:
                    logger.warning(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {status}")
                    
                    # ì‹¤íŒ¨í•œ ì‘ì—… ë¶„ì„
                    jobs = self.get_pipeline_jobs(pipeline_id)
                    failed_jobs = [job for job in jobs if job['status'] == 'failed']
                    
                    for job in failed_jobs:
                        logger.error(f"ğŸ’¥ ì‹¤íŒ¨í•œ ì‘ì—…: {job['name']}")
                        job_log = self.get_job_log(job['id'])
                        
                        if job_log:
                            failure_analysis = self.analyze_failure(job_log)
                            logger.info(f"ğŸ” ì‹¤íŒ¨ ë¶„ì„: {failure_analysis['type']} (ì‹ ë¢°ë„: {failure_analysis['confidence']*100:.1f}%)")
                            
                            # ìë™ ìˆ˜ì • ì‹œë„
                            if self.auto_fix_pipeline(pipeline_id, failure_analysis):
                                # ìˆ˜ì • í›„ ìƒˆ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°
                                new_pipeline_id = self.trigger_pipeline()
                                if new_pipeline_id:
                                    return self.monitor_pipeline(new_pipeline_id, max_wait_time - (time.time() - start_time))
                    
                    return False
                
                elif status in ['running', 'pending']:
                    # ê³„ì† ëª¨ë‹ˆí„°ë§
                    time.sleep(check_interval)
                    continue
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(check_interval)
        
        logger.warning("â° ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì´ˆê³¼")
        return False
    
    def run_autonomous_monitoring(self):
        """ì™„ì „ ììœ¨ì  ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        logger.info("ğŸ¤– ììœ¨ì  íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # ìµœì‹  íŒŒì´í”„ë¼ì¸ í™•ì¸
        pipeline = self.get_latest_pipeline()
        
        if not pipeline:
            logger.error("âŒ ëª¨ë‹ˆí„°ë§í•  íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        pipeline_id = pipeline['id']
        current_status = pipeline['status']
        
        logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ {pipeline_id} í˜„ì¬ ìƒíƒœ: {current_status}")
        
        if current_status in ['running', 'pending']:
            # ì‹¤í–‰ ì¤‘ì¸ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§
            return self.monitor_pipeline(pipeline_id)
        
        elif current_status == 'failed':
            # ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ ìë™ ìˆ˜ì • ì‹œë„
            logger.warning("âš ï¸ ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ ê°ì§€ - ìë™ ìˆ˜ì • ì‹œë„")
            
            jobs = self.get_pipeline_jobs(pipeline_id)
            failed_jobs = [job for job in jobs if job['status'] == 'failed']
            
            for job in failed_jobs:
                job_log = self.get_job_log(job['id'])
                failure_analysis = self.analyze_failure(job_log)
                
                if self.auto_fix_pipeline(pipeline_id, failure_analysis):
                    new_pipeline_id = self.trigger_pipeline()
                    if new_pipeline_id:
                        return self.monitor_pipeline(new_pipeline_id)
            
            return False
        
        elif current_status == 'success':
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ì´ ì´ë¯¸ ì„±ê³µ ìƒíƒœì…ë‹ˆë‹¤")
            return True
        
        else:
            logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ íŒŒì´í”„ë¼ì¸ ìƒíƒœ: {current_status}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        monitor = PipelineMonitor()
        
        logger.info("ğŸš€ FortiGate Nextrade GitLab CI/CD ììœ¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        success = monitor.run_autonomous_monitoring()
        
        if success:
            logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ - ì„±ê³µ!")
            sys.exit(0)
        else:
            logger.error("âŒ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()